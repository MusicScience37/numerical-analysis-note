% !TEX root = ../main.tex
%

\chapter{カーネル法}

\index{かーねるほう@カーネル法}
本章では，カーネルを用いて補間を行うカーネル法についてまとめる．

関数 $f : X \to \setC$ を
サンプル点 $(\bm{r}_i, y_i) \in X \times \setC$ ($i = 1, 2, \ldots, m$) から補間することを考える．
カーネルを用いた補間では，カーネル $K : X \times X \to \setC$ を用いて
\begin{equation}
    f(\bm{r}) = \sum_{i=1}^m c_i K(\bm{r}, \bm{r}_i)
\end{equation}
のようにおき，$y_i = f(\bm{r}_i)$ となるように係数 $c_i$ を決める
\cite{Fukumizu2010}．

\section{Tikhonov 正則化による導出}\label{sec:interp_kernel_tikhonov}

まず，カーネルによる補間の導出を行う．

関数 $f$ はある正規直交基底の存在する関数空間 $\mathcal{H}$
\footnote{正確には，関数空間 $\mathcal{H}$ には%
    再生核ヒルベルト空間 (reproducing kernel Hilbert space, RKHS)%
    と呼ばれるものを用いる．}
に属するものとし，
その関数空間 $\mathcal{H}$ には正規直交基底となる
関数 $\alpha_1(\bm{r}), \alpha_2(\bm{r}), \ldots, \alpha_N(\bm{r})$ が存在するものとする
\footnote{この導出では $N$ が有限であるとする．}．
それらの基底を用いて関数 $f$ を次のようにおく．
\begin{equation}
    f(\bm{r}) = \sum_{i = 1}^{N} x_i \alpha_i(\bm{r})
\end{equation}
ここで，
$A_{ij} = \alpha_j(\bm{r}_i)$ となる行列 $A \in \setC^{m \times N}$ を考えると，
最小二乗法の評価関数は
\begin{equation}
    \|A \bm{x} - \bm{y}\|_2
\end{equation}
となる．

これに Tikhonov 正則化（\ref{chap:regularization_tikhonov} 章）を適用する．
関数空間 $\mathcal{H}$ のノルムを $\|\cdot\|_{\mathcal{H}}$ とすると，
$\alpha_i$ が $\mathcal{H}$ の正規直交基底であることから，
\begin{align}
    \|f\|_{\mathcal{H}}^2
     & = \sum_{i = 1}^{N} |x_i|^2 \notag \\
     & = \|\bm{x}\|_2^2
\end{align}
となる．
よって，Tikhonov 正則化の評価関数は
式 \eqref{eq:regularization_tikhonov_objective} のように書ける．

式 \eqref{eq:regularization_tikhonov_exchange-mat} より最適解は
\begin{align}
    \bm{x} & = A^* (AA^* + \lambda I)^{-1} \bm{y}
\end{align}
となる．
ここで，行列 $AA^*$ の $(i, j)$ 要素は
\begin{equation}
    [AA^*]_{ij} = \sum_{k = 1}^{N} \alpha_k(\bm{r}_i) \overline{\alpha_k(\bm{r}_j)}
\end{equation}
となる．それをもとに次のように関数 $K : X \times X \to \setC$ を定義する．
\begin{equation}
    K(\bm{r}, \bm{r}') = \sum_{k = 1}^{N} \alpha_k(\bm{r}) \overline{\alpha_k(\bm{r}')}
\end{equation}
この関数がカーネルである．
また，行列 $K_m \in \setC^{m \times m}$ を
$K_m = AA^*$ のようにおく．
なお，この行列は半正定値エルミート行列である．

ここで，変数 $\bm{c} \in \setC^m$ を
\begin{equation}
    \bm{c} = (K_m + \lambda I)^{-1} \bm{y}
    \label{eq:interp_kernel_tikhonov_coeff_c}
\end{equation}
のようにおく．
このとき，$\bm{x} = A^* \bm{c}$ だから，
\begin{align}
    f(\bm{r})
     & = \sum_{i = 1}^{N} x_i \alpha_i(\bm{r}) \notag                                                \\
     & = \sum_{i = 1}^{N} \sum_{j = 1}^{m} \overline{A_{ji}} c_j \alpha_i(\bm{r}) \notag             \\
     & = \sum_{i = 1}^{N} \sum_{j = 1}^{m} \overline{\alpha_i(\bm{r}_j)} c_j \alpha_i(\bm{r}) \notag \\
     & = \sum_{j = 1}^{m} c_j K(\bm{r}, \bm{r}_j)
\end{align}
となる．

関数 $f$ を正規直交基底で表していたことから，
カーネル法は関数空間 $\mathcal{H}$ 全体の中で Tikhonov 正則化の評価関数
\begin{equation}
    \sum_{i=1}^m \left|y_i - f(\bm{r}_i)\right|^2
    + \|f\|_{\mathcal{H}}^2
\end{equation}
を最小化したものを求められるということが分かる．
また，ここでは導出において関数空間 $\mathcal{H}$ の次元を表す $N$ が有限であることを前提としていたが，
導出されたカーネル法で用いる行列は $m \times m$ の正方行列であるため，
$N \to \infty$ の場合にも適用できる．

\section{Gaussian Process}\label{sec:regularization_kernel_gaussian-process}

カーネルによる補間は，
\index{Gaussian Process}
Gaussian Process と呼ばれる確率分布から導くこともできる
\cite{Brochu2010}．
ただし，データとカーネル関数の値は実数とする．

平均 $\mu(\bm{r})$，分散 $K(\bm{r}, \bm{r}')$ の Gaussian Process に従う関数 $f$ は，
関数値のベクトル $(f(\bm{r}_1), f(\bm{r}_2), \ldots, f(\bm{r}_m))$ が
正規分布 $\mathcal{N}(\bm{\mu}_m, K_m)$ に従う．
ここで，
$\bm{\mu}_m$ は $[\bm{\mu}_m]_i = \mu(\bm{r}_i)$ なるベクトルであり，
$K_m$ は $[K_m]_{ij} = K(\bm{r}_i, \bm{r}_j)$ なる行列である．
Gaussian Process は関数における正規分布のようなものである．

関数 $f$ が平均 $0$，分散 $\tau K(\bm{r}, \bm{r}')$ の Gaussian Process に従っており，
関数 $f$ に対するノイズ入りのサンプルが $y_i = f(\bm{r}_i) + \epsilon_i$ のように得られているとする．
ただし，$\epsilon_i$ は独立に正規分布 $\mathcal{N}(0, \sigma^2)$ に従うものとする．
また，$\tau > 0$ は分散の大きさを示すパラメータである．
このとき，ベクトル $(y_1, y_2, \ldots, y_m, f(\bm{r}))$ は
次のような正規分布に従う．
\begin{equation}
    \mathcal{N}\left(\bm{0},
    \begin{pmatrix}
        \tau K_m + \sigma^2 I & \tau \bm{k}(\bm{r})    \\
        \tau \bm{k}(\bm{r})^T & \tau K(\bm{r}, \bm{r})
    \end{pmatrix}
    \right)
\end{equation}
このとき，確率密度関数 $p(y_1, y_2, \ldots, y_m, f(\bm{r}))$ は次のように書ける．
\begin{equation}
    p(y_1, y_2, \ldots, y_m, f(\bm{r}))
    = \frac{1}{\sqrt{(2\pi)^{m+1} \det{
                \begin{pmatrix}
                    \tau K_m + \sigma^2 I & \tau \bm{k}(\bm{r})    \\
                    \tau \bm{k}(\bm{r})^T & \tau K(\bm{r}, \bm{r})
                \end{pmatrix}
            }}}
    \exp\left(-\frac{1}{2}
    \begin{pmatrix}
        \bm{y} \\ f(\bm{r})
    \end{pmatrix}^T
    \begin{pmatrix}
        \tau K_m + \sigma^2 I & \tau \bm{k}(\bm{r})    \\
        \tau \bm{k}(\bm{r})^T & \tau K(\bm{r}, \bm{r})
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        \bm{y} \\ f(\bm{r})
    \end{pmatrix}
    \right)
\end{equation}

ここで，
\begin{align}
    \sigma_K^2(\bm{r})
     & \equiv \tau K(\bm{r}, \bm{r})
    - \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1} \tau \bm{k}(\bm{r}) \\
     & = \tau K(\bm{r}, \bm{r})
    - \tau \bm{k}(\bm{r})^T (K_m + \sigma^2 I)^{-1} \bm{k}(\bm{r})
\end{align}
とおくと，
\begin{align}
    \det{
        \begin{pmatrix}
            \tau K_m + \sigma^2 I & \tau \bm{k}(\bm{r})    \\
            \tau \bm{k}(\bm{r})^T & \tau K(\bm{r}, \bm{r})
        \end{pmatrix}
    }
     & = \sigma_K^2(\bm{r}) \det(\tau K_m + \sigma^2 I)
\end{align}
となる．
また，
\begin{align}
     & \hphantom{=}
    \begin{pmatrix}
        \tau K_m + \sigma^2 I & \tau \bm{k}(\bm{r})    \\
        \tau \bm{k}(\bm{r})^T & \tau K(\bm{r}, \bm{r})
    \end{pmatrix}^{-1}
    \notag          \\
     & =
    \begin{pmatrix}
        (\tau K_m + \sigma^2 I)^{-1}
        + (\tau K_m + \sigma^2 I)^{-1} \tau \bm{k}(\bm{r}) \sigma_K^2(\bm{r})^{-1} \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1}
         &
        - (\tau K_m + \sigma^2 I)^{-1} \tau \bm{k}(\bm{r}) \sigma_K^2(\bm{r})^{-1}
        \\
        - \sigma_K^2(\bm{r})^{-1} \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1}
         &
        \sigma_K^2(\bm{r})^{-1}
    \end{pmatrix}
\end{align}
であり，
\begin{align}
     & \hphantom{=}
    \begin{pmatrix}
        \bm{y} \\ f(\bm{r})
    \end{pmatrix}^T
    \begin{pmatrix}
        \tau K_m + \sigma^2 I & \tau \bm{k}(\bm{r})    \\
        \tau \bm{k}(\bm{r})^T & \tau K(\bm{r}, \bm{r})
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        \bm{y} \\ f(\bm{r})
    \end{pmatrix}
    \notag                                            \\
     & = \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
    + \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \tau \bm{k}(\bm{r}) \sigma_K^2(\bm{r})^{-1} \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
    \notag                                            \\ & \hspace{2em}
    - f(\bm{r}) \sigma_K^2(\bm{r})^{-1} \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
    - \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \tau \bm{k}(\bm{r}) \sigma_K^2(\bm{r})^{-1} f(\bm{r})
    \notag                                            \\ & \hspace{2em}
    + f(\bm{r}) \sigma_K^2(\bm{r})^{-1} f(\bm{r})
    \notag                                            \\
     & = \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
    + \sigma_K^2(\bm{r})^{-1} \left(f(\bm{r}) - \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}\right)^2
\end{align}
となる．
よって，確率密度関数は次のように分割できる．
\begin{align}
    p(y_1, y_2, \ldots, y_m, f(\bm{r}))
     & = p_{\bm{y}}(\bm{y}) p_f(f(\bm{r}))
    \\
    p_{\bm{y}}(\bm{y})
     & = \frac{1}{\sqrt{(2\pi)^{m} \det(\tau K_m + \sigma^2 I)}}
    \exp\left(-\frac{1}{2} \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \bm{y} \right)
    \\
    p_f(f(\bm{r}))
     & = \frac{1}{\sqrt{(2\pi)^{m} \sigma_K^2(\bm{r})}}
    \exp\left(-\frac{1}{2} \sigma_K^2(\bm{r})^{-1} \left(f(\bm{r}) - \mu_K(\bm{r})\right)^2\right)
    \\
    \mu_K(\bm{r})
     & = \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
\end{align}
分割したあとの
$p_{\bm{y}}$ は $\bm{y}$ に関する正規分布 $\mathcal{N}(\bm{0}, \tau K_m + \sigma^2 I)$ であり，
$p_f$ は $f(\bm{r})$ に関する正規分布 $\mathcal{N}(\mu_K(\bm{r}), \sigma_K^2(\bm{r}))$ である．
$\lambda = \sigma^2 / \tau$ とおくと，
\begin{align}
    \mu_K(\bm{r})
     & = \tau \bm{k}(\bm{r})^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
    \notag                                                                               \\
     & = \bm{k}(\bm{r})^T (K_m + \lambda I)^{-1} \bm{y}
    \notag                                                                               \\
     & = \sum_{i = 1}^m K(\bm{r}, \bm{r}_i) \left[(K_m + \lambda I)^{-1} \bm{y}\right]_i
\end{align}
のように前節と同様の公式が得られる．

Gaussian Process を用いた表現では分散 $\sigma_K^2(\bm{r})$ も得られるため，
補間された関数のどの部分に誤差が多い可能性が高いか評価するのにも役立つ．
また，パラメータ推定において確率の最大化を行うといった応用もある
（\ref{sec:regularization_kernel_param-est} 節）．

\section{RBF 補間}\label{sec:interp_kernel_rbf}

\index{RBFほかん@RBF 補間}
\index{Radial Basis Function}
カーネル $K(\bm{r}, \bm{r}')$ の値が距離 $\|\bm{r} - \bm{r}'\|_2$ に依存する場合，
そのもとになる関数は Radial Basis Function (RBF) と呼ばれる．
RBF $\phi : [0, \infty) \to \setR$ によるカーネルは
\begin{equation}
    K(\bm{r}, \bm{r}') = \phi\left(\frac{\|\bm{r} - \bm{r}'\|_2}{c}\right)
    \label{eq:regularization_kernel_kernel-of-rbf}
\end{equation}
のように書ける．
RBF としては表
\ref{table:interp_kernel_example-rbfs}
のようなものが挙げられる
\cite{Brochu2010,Fornberg2015}．

\begin{table}[bp]
    \caption{RBF の例 \cite{Brochu2010,Fornberg2015}}
    \label{table:interp_kernel_example-rbfs}
    \centering
    \begin{tabular}{ll}
        名称                      & 関数                         \\
        \hline
        Gaussian                & $e^{-r^2}$                 \\
        Multi-quadric           & $\sqrt{1 + r^2}$           \\
        Inverse Multi-quadric   & $1/\sqrt{1 + r^2}$         \\
        Inverse Quadric         & $1 / (1 + r^2)$            \\
        Bessel ($d=1,2,\ldots$) & $J_{d/2-1}(r) / r^{d/2-1}$
    \end{tabular}
\end{table}

\section{パラメータ推定}\label{sec:regularization_kernel_param-est}

式 \eqref{eq:regularization_kernel_kernel-of-rbf} の定数 $c$ のように
カーネルがパラメータを持っている場合がある．
そのようなパラメータの推定を
\cite[Remark 3 (Connection to spatial statistics)]{Scheuerer2011}
に沿った
\index{maximum likelihood estimation}
maximum likelihood estimation (MLE)
により考える．

Gaussian Process （\ref{sec:regularization_kernel_gaussian-process} 節）より，
データ $\bm{y}$ の確率密度関数は
\begin{equation}
    p(\bm{y}) = \frac{1}{\sqrt{(2\pi)^{m} \det(\tau K_m + \sigma^2 I)}}
    \exp\left(-\frac{1}{2} \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \bm{y} \right)
\end{equation}
であり，その対数を取ると
\begin{equation}
    \log{p(\bm{y})}
    = -\frac{m}{2}\log(2\pi)
    - \frac{1}{2} \log{\det(\tau K_m + \sigma^2 I)}
    - \frac{1}{2} \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
\end{equation}
となる．
これを最大化するようにパラメータを決定する．

$\lambda = \sigma^2 / \tau$ を固定すると，
\begin{equation}
    \log{p(\bm{y})}
    = -\frac{m}{2}\log(2\pi)
    - \frac{m}{2} \log{\tau}
    - \frac{1}{2} \log{\det(K_m + \lambda I)}
    - \frac{1}{2\tau} \bm{y}^T (K_m + \lambda I)^{-1} \bm{y}
\end{equation}
となる．
これを最大化するように $\tau$ を決定すると，
\begin{equation}
    \tau = \frac{1}{m} \bm{y}^T (K_m + \lambda I)^{-1} \bm{y}
    \label{eq:interp_kernel_param_coeff_tau}
\end{equation}
となり，そのときの $\log{p(\bm{y})}$ は
\begin{equation}
    \log{p(\bm{y})}
    = -\frac{m}{2}\log(2\pi)
    + \frac{m}{2} \log{m}
    - \frac{m}{2} \log(\bm{y}^T (K_m + \lambda I)^{-1} \bm{y})
    - \frac{1}{2} \log{\det(K_m + \lambda I)}
    - \frac{m}{2}
\end{equation}
となる．
$m$ はサンプルの数であり，定数だから，
パラメータ推定では
\begin{equation}
    m \log(\bm{y}^T (K_m + \lambda I)^{-1} \bm{y})
    + \log{\det(K_m + \lambda I)}
\end{equation}
を最小化すれば良い．

\section{固有値分解による数値解法}

密行列の固有値分解を用いてカーネル法の計算を行うことを考える．

カーネルの値を並べた行列 $K_m \in \setC^{m \times m}$ を次のように固有値分解する．
\begin{equation}
    K_m = V D V^*
\end{equation}
ここで，$V \in \setC^{m \times m}$ はエルミート行列であり，
$D \in \setR^{m \times m}$ は実数による対角行列である．
ここで，$K_m$ は半正定値とする
（このような性質を持つカーネルは正定値カーネルと呼ばれる \cite{Fukumizu2010}）．
\ref{sec:interp_kernel_tikhonov} 節で導出したカーネルや
\ref{sec:interp_kernel_rbf} 節で示した Gaussian の RBF によるカーネルは
正定値カーネルである．
このとき，
\begin{equation}
    K_m + \lambda I = V (D + \lambda I) V^*
\end{equation}
が成り立つ．
$\lambda > 0$ であれば確実に $K_m + \lambda I$ が正定値となり，逆行列を持つ．
よって，
式 \eqref{eq:interp_kernel_tikhonov_coeff_c} で定めた係数 $\bm{c}$ は
\begin{align}
    \bm{c}
     & = (K_m + \lambda I)^{-1} \bm{y} \notag \\
     & = V (D + \lambda I)^{-1} V^* \bm{y}
\end{align}
と書ける．
さらに，
$V = (\bm{v}_1, \bm{v}_2, \ldots, \bm{v}_m)$,
$D = \diag(\lambda_1, \lambda_2, \ldots, \lambda_m)$
とおくと，
\begin{align}
    \bm{c}
     & = \sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^* \bm{y}) \bm{v}_i
\end{align}
のように表すこともできる．

なお，$K_m$, $\bm{y}$ が実数の行列とベクトルである場合，
パラメータ推定における
式 \eqref{eq:interp_kernel_param_coeff_tau} の $\tau$ は
\begin{align}
    \tau
     & = \frac{1}{m} \bm{y}^T (K_m + \lambda I)^{-1} \bm{y} \notag                    \\
     & = \frac{1}{m} \sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^T \bm{y})^2
\end{align}
のように書ける．
さらに，評価関数は
\begin{align}
     & \hphantom{=}
    m \log(\bm{y}^T (K_m + \lambda I)^{-1} \bm{y})
    + \log{\det(K_m + \lambda I)}
    \notag                                                                                   \\
     & = m \log\left(\sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^T \bm{y})^2\right)
    + \log\left(\prod_{i=1}^m (\lambda_i + \lambda)\right)
    \notag                                                                                   \\
     & = m \log\left(\sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^T \bm{y})^2\right)
    + \sum_{i=1}^m \log(\lambda_i + \lambda)
\end{align}
のように書ける．

\section{大域最適解への応用}\label{sec:interp_kernel_global-optimization}

Gaussian Process （\ref{sec:regularization_kernel_gaussian-process} 節）
を利用して大域最適解を行う
Gaussian Process 最適化
\index{Gaussian Process さいてきか@Gaussian Process 最適化}
が考案されている
\cite{Srinivas2010}．

領域 $D \in \setR^d$ 上で関数 $f : D \to \setR$ を最小化する
制約なし最適化の問題を考える
\footnote{文献 \cite{Srinivas2010} では最大化を前提としているが，%
    ここでは \ref{part:optimization} 部に合わせて最小化を考える．}．
文献 \cite{Srinivas2010} のアルゴリズムにおいては，
各反復ごとにこれまでに算出したサンプル点
$(\bm{x}_i, f(\bm{x}_i))$ ($i = 1,2,\ldots,T$)
に対して Gaussian Process による補間を行い，
Gaussian Process の平均 $\mu_T(\bm{x})$ と標準偏差 $\sigma_T(\bm{x})$ を算出できるようにする．
そして，反復ごとに変化する係数 $\beta_t$ とパラメータ $\nu$ を用いて定義される目的関数
\begin{equation}
    F_T(\bm{x}) = \mu_T(\bm{x}) - \sqrt{\nu \beta_{T+1}} \sigma_T(\bm{x})
\end{equation}
を最小化するような $\bm{x}$ を次のサンプル点として選択する．
係数 $\nu$, $\beta_t$ は
\begin{itemize}
    \item $\nu = 1/5$, $\beta_t = 2 \log(|D| t^2 \pi^2 / 6 \delta)$,
          $\delta \in (0, 1)$
          \cite{Srinivas2010}
          \footnote{文献 \cite{Srinivas2010} 内の定理では $\nu = 1$ の場合を扱っていたが，%
              数値実験において $\nu = 1/5$ とすると結果が良くなったという記述がある．%
              また，文献 \cite{Srinivas2010} の数値実験では $\delta = 0.1$ が選択されている．}
    \item $\nu=1$, $\beta_t = 2 \log(t^{d/2+2} \pi^2 /3 \delta)$,
          $\delta \in (0, 1)$
          \cite{Brochu2010}
\end{itemize}
のように与えられる．
関数 $F_T$ を最小化するために別の大域最適解のアルゴリズムが必要となるが，
元の目的関数 $f$ の値を得るのに時間がかかる場合には有効なアルゴリズムである．
