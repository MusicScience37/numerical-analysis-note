% !TEX root = ../../main.tex
%

\section{固有値分解による数値解法}\label{sec:interp_kernel_evd-computation}

密行列の固有値分解を用いてカーネル法の計算を行うことを考える．

\subsection{追加の項がない場合}

まず，本節では \ref{sec:interp_kernel_additional-terms} 節のような追加の項がない場合の
方程式 $K_m \bm{c} = \bm{y}$ を考える．
カーネルの値を並べた行列 $K_m \in \setC^{m \times m}$ を次のように固有値分解する．
\begin{equation}
    K_m = V D V^*
\end{equation}
ここで，$V \in \setC^{m \times m}$ はエルミート行列であり，
$D \in \setR^{m \times m}$ は実数による対角行列である．
ここで，$K_m$ は半正定値とする
（このような性質を持つカーネルは正定値カーネルと呼ばれる \cite{Fukumizu2010}）．
\ref{sec:interp_kernel_tikhonov} 節で導出したカーネルや
\ref{sec:interp_kernel_rbf} 節で示した Gaussian の RBF によるカーネルは
正定値カーネルである．
このとき，
\begin{equation}
    K_m + \lambda I = V (D + \lambda I) V^*
\end{equation}
が成り立つ．
$\lambda > 0$ であれば確実に $K_m + \lambda I$ が正定値となり，逆行列を持つ．
よって，
式 \eqref{eq:interp_kernel_tikhonov_coeff_c} で定めた係数 $\bm{c}$ は
\begin{align}
    \bm{c}
     & = (K_m + \lambda I)^{-1} \bm{y} \notag \\
     & = V (D + \lambda I)^{-1} V^* \bm{y}
\end{align}
と書ける．
さらに，
$V = (\bm{v}_1, \bm{v}_2, \ldots, \bm{v}_m)$,
$D = \diag(\lambda_1, \lambda_2, \ldots, \lambda_m)$
とおくと，
\begin{align}
    \bm{c}
     & = \sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^* \bm{y}) \bm{v}_i
\end{align}
のように表すこともできる．

\subsection{追加の項がある場合}

続いて，本節では \ref{sec:interp_kernel_additional-terms} 節のような追加の項がある場合の
方程式
\begin{equation}
    \begin{pmatrix}
        K_m + \lambda I & P \\
        P^\top          & O
    \end{pmatrix}
    \begin{pmatrix}
        \bm{c} \\ \bm{d}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \bm{y} \\ \bm{0}
    \end{pmatrix}
\end{equation}
を考える．
ただし，サンプル点と追加の項の関数 $p_i$ を適切に選択することにより
行列 $P$ のランクが $M$ となるようにしておく．
また，行列 $K_m$ は実数による行列で正定値とする．

追加で出てきた行列 $P$ について以下のように QR 分解を行う．
\begin{equation}
    P =
    \begin{pmatrix}
        Q_1 & Q_2
    \end{pmatrix}
    \begin{pmatrix}
        R \\ O
    \end{pmatrix}
\end{equation}
ここで，
$Q_1 \in \setR^{m \times M}$,
$Q_2 \in \setR^{m \times (m-M)}$
$R \in \setR^{M \times M}$
とする．
また，行列 $P$ のランクが $M$ としているため，行列 $R$ は正則行列となる．

方程式のうち $P^\top \bm{c} = \bm{0}$ より，
\begin{align}
    \bm{c}^\top P     & = \bm{0} \notag \\
    \bm{c}^\top Q_1 R & = \bm{0} \notag \\
    \bm{c}^\top Q_1   & = \bm{0}
\end{align}
となるため，$\bm{c}$ は $Q_1$ のすべての列に対して直行するように選択する必要がある．
QR 分解において $(Q_1, Q_2)$ は直行行列になることから，
$\bm{c}$ は $Q_2$ の列ベクトルにより作られる部分空間に属している必要があるため，
$\bm{c} = Q_2 \bm{\gamma}$ のようにベクトル $\bm{\gamma}$ を用いて書ける．

方程式の残りの $(K_m + \lambda I) \bm{c} + P \bm{d} = \bm{y}$ について，
変数を置き換えると
\begin{align}
    (K_m + \lambda I) Q_2 \bm{\gamma} + Q_1 R \bm{d} = \bm{y}
\end{align}
となる．
左から $Q_2^\top$ をかけると，$Q_2^\top Q_1 = O$ より
\begin{align}
    Q_2^\top (K_m + \lambda I) Q_2 \bm{\gamma} & = Q_2^\top \bm{y}
\end{align}
となる．$(K_m + \lambda I)$ が正定値となることから，
フルランクの行列 $Q_2$ をかけた $Q_2^\top (K_m + \lambda I) Q_2$ も正定値（つまり正則でもある）となり，
\begin{equation}
    \bm{\gamma} = \left( Q_2^\top (K_m + \lambda I) Q_2 \right)^{-1} Q_2^\top \bm{y}
\end{equation}
となる．
よって，$\bm{c} = Q_2 \bm{\gamma}$ より
\begin{equation}
    \bm{c} = Q_2 \left( Q_2^\top (K_m + \lambda I) Q_2 \right)^{-1} Q_2^\top \bm{y}
\end{equation}
となる．

また，$(K_m + \lambda I) \bm{c} + P \bm{d} = \bm{y}$ より
\begin{align}
    P \bm{d}     & = \bm{y} - (K_m + \lambda I) \bm{c} \notag                                \\
    Q_1 R \bm{d} & = \bm{y} - (K_m + \lambda I) \bm{c} \notag                                \\
    R \bm{d}     & = Q_1^\top \left( \bm{y} - (K_m + \lambda I) \bm{c} \right) \notag        \\
    \bm{d}       & = R^{-1} Q_1^\top \left( \bm{y} - (K_m + \lambda I) \bm{c} \right) \notag \\
    \bm{d}       & = R^{-1} Q_1^\top \left( \bm{y} - K_m \bm{c} \right)
\end{align}
となる．
Moore-Penrose の一般化逆行列を用いて
$\bm{d} = R^\dagger \left( \bm{y} - K_m \bm{c} \right)$
としても良い
\footnote{処理系が QR 分解の結果を用いて方程式を解くようなプログラムを用意している場合がある．}．

さらに，$Q_2^\top K_m Q_2$ の固有値分解
$Q_2^\top K_m Q_2 = U D U^\top$ （$D$ は対角行列で $U$ は直行行列）が得られている場合，
\begin{align}
    \bm{c}
     & = Q_2 \left( Q_2^\top (K_m + \lambda I) Q_2 \right)^{-1} Q_2^\top \bm{y}
    \notag                                                                      \\
     & = Q_2 \left( Q_2^\top K_m Q_2 + \lambda I \right)^{-1} Q_2^\top \bm{y}
    \notag                                                                      \\
     & = Q_2 \left( U D U^\top + \lambda I \right)^{-1} Q_2^\top \bm{y}
    \notag                                                                      \\
     & = Q_2 U \left( D + \lambda I \right)^{-1} U^\top Q_2^\top \bm{y}
    \notag                                                                      \\
     & = Q_2 U \left( D + \lambda I \right)^{-1} (Q_2 U)^\top \bm{y}
\end{align}
のように計算できる．

\section{パラメータ推定}\label{sec:regularization_kernel_param-est}

式 \eqref{eq:regularization_kernel_kernel-of-rbf} の定数 $c$ のように
カーネルがパラメータを持っている場合がある．
そのようなパラメータの推定を
\cite[Remark 3 (Connection to spatial statistics)]{Scheuerer2011}
に沿った
\index{maximum likelihood estimation}
maximum likelihood estimation (MLE)
により考える．
なお，本節ではカーネルの値やデータの値がすべて実数になっているものとする．

Gaussian Process （\ref{sec:regularization_kernel_gaussian-process} 節）より，
データ $\bm{y}$ の確率密度関数は
\begin{equation}
    p(\bm{y}) = \frac{1}{\sqrt{(2\pi)^{m} \det(\tau K_m + \sigma^2 I)}}
    \exp\left(-\frac{1}{2} \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \bm{y} \right)
\end{equation}
であり，その対数を取ると
\begin{equation}
    \log{p(\bm{y})}
    = -\frac{m}{2}\log(2\pi)
    - \frac{1}{2} \log{\det(\tau K_m + \sigma^2 I)}
    - \frac{1}{2} \bm{y}^T (\tau K_m + \sigma^2 I)^{-1} \bm{y}
\end{equation}
となる．
これを最大化するようにパラメータを決定する．

$\lambda = \sigma^2 / \tau$ を固定すると，
\begin{equation}
    \log{p(\bm{y})}
    = -\frac{m}{2}\log(2\pi)
    - \frac{m}{2} \log{\tau}
    - \frac{1}{2} \log{\det(K_m + \lambda I)}
    - \frac{1}{2\tau} \bm{y}^T (K_m + \lambda I)^{-1} \bm{y}
\end{equation}
となる．
これを最大化するように $\tau$ を決定すると，
\begin{equation}
    \tau = \frac{1}{m} \bm{y}^T (K_m + \lambda I)^{-1} \bm{y}
    \label{eq:interp_kernel_param_coeff_tau}
\end{equation}
となり，そのときの $\log{p(\bm{y})}$ は
\begin{equation}
    \log{p(\bm{y})}
    = -\frac{m}{2}\log(2\pi)
    + \frac{m}{2} \log{m}
    - \frac{m}{2} \log(\bm{y}^T (K_m + \lambda I)^{-1} \bm{y})
    - \frac{1}{2} \log{\det(K_m + \lambda I)}
    - \frac{m}{2}
\end{equation}
となる．
$m$ はサンプルの数であり，定数だから，
パラメータ推定では
\begin{equation}
    m \log(\bm{y}^T (K_m + \lambda I)^{-1} \bm{y})
    + \log{\det(K_m + \lambda I)}
\end{equation}
を最小化すれば良い．

なお，
\ref{sec:interp_kernel_evd-computation} 節における固有値分解を用いることで，
パラメータ推定における
式 \eqref{eq:interp_kernel_param_coeff_tau} の $\tau$ は
\begin{align}
    \tau
     & = \frac{1}{m} \bm{y}^T (K_m + \lambda I)^{-1} \bm{y} \notag                    \\
     & = \frac{1}{m} \sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^T \bm{y})^2
\end{align}
のように書ける．
さらに，評価関数は
\begin{align}
     & \hphantom{=}
    m \log(\bm{y}^T (K_m + \lambda I)^{-1} \bm{y})
    + \log{\det(K_m + \lambda I)}
    \notag                                                                                   \\
     & = m \log\left(\sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^T \bm{y})^2\right)
    + \log\left(\prod_{i=1}^m (\lambda_i + \lambda)\right)
    \notag                                                                                   \\
     & = m \log\left(\sum_{i=1}^m \frac{1}{\lambda_i + \lambda} (\bm{v}_i^T \bm{y})^2\right)
    + \sum_{i=1}^m \log(\lambda_i + \lambda)
\end{align}
のように書ける．
