% !TEX root = ../../main.tex
%

\section[L1 正則化]{$L_1$ 正則化}
\index{L1せいそくか@$L_1$ 正則化}

本節では，最も単純な 1-ノルムによる正則化（$L_1$ 正則化）による評価関数
\begin{equation}
    E_{\lambda}(\bm{x}) \equiv \|A \bm{x} - \bm{y}\|_2^2 + \lambda \|\bm{x}\|_1
    \label{eq:regularization_sparse_l1_objective}
\end{equation}
を考える．
このような正則化は lasso とも呼ばれる \cite{Boyd2010}．
この評価関数の最小化を行うアルゴリズムについて以下にまとめる．

\subsection{交互方向乗数法}
\index{こうごほうこうじょうすうほう@交互方向乗数法}

式 \eqref{eq:regularization_sparse_l1_objective} の評価関数の最小化を
\begin{equation}
    \begin{aligned}
        \text{minimize} \hspace{1em} & \|A \bm{x} - \bm{y}\|_2^2 + \lambda \|\bm{z}\|_1 \\
        \text{s.t.} \hspace{1em}     & \bm{x} - \bm{z} = \bm{0}
    \end{aligned}
\end{equation}
と書き換えることで，
交互方向乗数法（\ref{sec:optimization_admm} 節）により
評価関数を最小化するような解 $\bm{x}$ を求めることができる \cite{Boyd2010}．

このとき，拡張ラグランジュ関数は
\begin{equation}
    L_{\rho}(\bm{x}, \bm{z}, \bm{p}) \equiv
    \|A \bm{x} - \bm{y}\|_2^2 + \lambda \|\bm{z}\|_1
    + \bm{p}^\top (\bm{x} - \bm{z})
    + \frac{\rho}{2} \|\bm{x} - \bm{z}\|_2^2
\end{equation}
となる．
ここで，ベクトル $\bm{p}$ はラグランジュ乗数，
$\rho$ は正の定数である．

交互方向乗数法における $\bm{x}$ の更新式は，
\begin{align}
    \bm{x}_{k+1}
     & = \argmin_{\bm{x}} L_{\rho}(\bm{x}, \bm{z}_k, \bm{p}_k)
    \notag                                                                          \\
     & = \argmin_{\bm{x}} \left( \|A \bm{x} - \bm{y}\|_2^2 + \lambda \|\bm{z}_k\|_1
    + \bm{p}_k^\top (\bm{x} - \bm{z}_k)
    + \frac{\rho}{2} \|\bm{x} - \bm{z}_k\|_2^2 \right)
    \notag                                                                          \\
     & = \argmin_{\bm{x}} \left( \|A \bm{x} - \bm{y}\|_2^2
    + \bm{p}_k^\top (\bm{x} - \bm{z}_k)
    + \frac{\rho}{2} \|\bm{x} - \bm{z}_k\|_2^2 \right)
\end{align}
となる．
$\bm{x}$ については 2 次関数になっているため，
この最小化は陽的に解を求めることができ，以下のようになる
\footnote{行列 $A$ の列の数が多い場合は%
    \ref{sec:linear_krylov-subspace-method} 節の%
    CG 法などで $\bm{x}_{k+1}$ を求める．}．
\begin{align}
    \bm{x}_{k+1}
     & = (2 A^\top A + \rho I)^{-1} (2 A^\top \bm{y} - \bm{p}_k + \rho \bm{z}_k)
\end{align}

$\bm{z}$ の更新式は，
\begin{align}
    \bm{z}_{k+1}
     & = \argmin_{\bm{z}} L_{\rho}(\bm{x}_{k+1}, \bm{z}, \bm{p}_k)
    \notag                                                                              \\
     & = \argmin_{\bm{z}} \left( \|A \bm{x}_{k+1} - \bm{y}\|_2^2 + \lambda \|\bm{z}\|_1
    + \bm{p}_k^\top (\bm{x}_{k+1} - \bm{z})
    + \frac{\rho}{2} \|\bm{x}_{k+1} - \bm{z}\|_2^2 \right)
    \notag                                                                              \\
     & = \argmin_{\bm{z}} \left( \lambda \|\bm{z}\|_1
    + \bm{p}_k^\top (\bm{x}_{k+1} - \bm{z})
    + \frac{\rho}{2} \|\bm{x}_{k+1} - \bm{z}\|_2^2 \right)
    \notag                                                                              \\
     & = \argmin_{\bm{z}} \left( \lambda \|\bm{z}\|_1
    + \frac{\rho}{2} \left\|\bm{x}_{k+1} - \bm{z} + \frac{\bm{p}_k}{\rho} \right\|_2^2 \right)
    \notag                                                                              \\
     & = \mathcal{T}_{\lambda/\rho} \left( \bm{x}_{k+1} + \frac{\bm{p}_k}{\rho} \right)
\end{align}
となる．
（最後の変形は \ref{sec:regularization_shrinkage-operator} 節を参照．）

$\bm{p}$ の更新式まで含めると以下のようになる
\footnote{文献 \cite{Boyd2010} と定式化のときの係数が異なるため，%
    最終的な更新式の係数が異なっている．}．
\begin{align}
    \bm{x}_{k+1} & = (2 A^\top A + \rho I)^{-1} (2 A^\top \bm{y} - \bm{p}_k + \rho \bm{z}_k)
    \\
    \bm{z}_{k+1} & = \mathcal{T}_{\lambda/\rho} \left( \bm{x}_{k+1} + \frac{\bm{p}_k}{\rho} \right)
    \\
    \bm{p}_{k+1} & = \bm{p}_k + \rho (\bm{x}_{k+1} - \bm{z}_{k+1})
\end{align}

\subsection{FISTA}

式 \eqref{eq:regularization_sparse_l1_objective} の
評価関数の最小化のために考えられたアルゴリズムの 1 つに
FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) \cite{Beck2009}
\index{Fast Iterative Shrinkage-Thresholding Algorithm}
\index{FISTA|see{Fast Iterative Shrinkage-Thresholding Algorithm}}
がある．

FISTA においては，以下のような一般化した評価関数を考える \cite{Beck2009}．
\begin{equation}
    F(\bm{x}) \equiv f(\bm{x}) + g(\bm{x})
\end{equation}
ここで，
関数 $f : \setR^n \to \setR$ は凸関数であり，
連続微分可能で，
勾配が
\begin{equation}
    \|\nabla f(\bm{x}) - \nabla f(\bm{y})\|_2 \le L \|\bm{x} - \bm{y}\|_2
\end{equation}
のように定数 $L > 0$ で Lipschitz 連続になっているものとする．
また，関数 $g : \setR^n \to \setR$ は凸関数であり，
微分可能とは限らない連続関数である．
そして，関数 $F$ は $\setR^n$ 上に最小値を持つものとする．

\begin{algorithm}[t]
    \caption{FISTA \cite{Beck2009}}
    \label{alg:regularization_fista-with-constant-stepsize}
    \begin{algorithmic}[1]
        \Procedure{FISTA}{$\bm{x}_0$, $L$}
        \State $\bm{y}_1 \gets \bm{x}_0$
        \State $t_1 \gets 1$
        \State $k \gets 1$
        \Loop
        \State $\bm{x}_k \gets \bm{p}_L(\bm{y}_k)$
        \State $t_{k+1} \gets \frac{\displaystyle 1 + \sqrt{1 + 4 t_k^2}}{\displaystyle 2}$
        \State $\bm{y}_{k+1} \gets \bm{x}_k + \frac{\displaystyle t_k - 1}{\displaystyle t_{k+1}} (\bm{x}_k - \bm{x}_{k-1})$
        \EndLoop
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

この評価関数について，点 $\bm{y} \in \setR^n$ 周りで近似する関数
\begin{equation}
    Q_L(\bm{x}, \bm{y}) \equiv f(\bm{y})
    + \nabla f(\bm{y})^\top (\bm{x} - \bm{y})
    + \frac{L}{2} \|\bm{x} - \bm{y}\|_2^2 + g(\bm{x})
\end{equation}
を考え，それが最小になる点を求める関数
\begin{equation}
    \bm{p}_L(\bm{y}) \equiv \argmin_{\bm{x}} Q_L(\bm{x}, \bm{y})
\end{equation}
を定義する \cite{Beck2009}．
この関数は
\begin{equation}
    \bm{p}_L(\bm{y}) = \argmin_{\bm{x}} \left(
    g(\bm{x}) + \frac{L}{2} \left\|\bm{x}
    - \left( \bm{y} - \frac{1}{L} \nabla f(\bm{y}) \right) \right\|_2^2
    \right)
\end{equation}
とも書ける \cite{Beck2009}．
この関数を用いて
Algorithm \ref{alg:regularization_fista-with-constant-stepsize}
のように解 $\bm{x}_k$ を更新していく．
