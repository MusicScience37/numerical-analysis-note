% !TEX root = ../main.tex
%

\chapter{Generalized Cross Validation}

正則化パラメータを求めるための手法の 1 つに
Generalized Cross Validation (GCV) がある．
GCV では，次の式の最小化を行う
\cite{Wahba1990}．

\begin{equation}
    V(\lambda)
    = \frac{\frac{1}{m} \|A \bm{x}_\lambda - \bm{y}\|^2}
    {\left|\frac{1}{m} \tr\{I - P_A(\lambda)\}\right|^2}
    \label{eq:regularization_gcv_gcv-evaluation-function}
\end{equation}

ここで，$\bm{x}_\lambda$ は
正則化の式 \eqref{eq:regularization_intro_general-regularization} を最小化する
$\bm{x}$であり，
$P_A(\lambda)$ は influence matrix と呼ばれるもので，
データ $\bm{y}$ から解 $\bm{x}_\lambda$ を求める作用素を $A_\lambda^\#$ としたときに，
\begin{equation}
    P_A(\lambda) \equiv
    \frac{\partial A A_\lambda^\# \bm{y}}{\partial \bm{y}}
    \label{eq:regularization_gcv_influence-matrix}
\end{equation}
のように定義される．

\section{導出}

まず，GCV の基になっている Ordinary Cross Validation (OCV) を
文献 \cite{Wahba1990} に沿って示す．

OCV では次の関数を考える．
\begin{equation}
    V_0(\lambda) \equiv
    \frac{1}{m} \sum_{i=1}^m
    \left|y_i - A_i \bm{x}_\lambda^{(i)}\right|^2
    \label{eq:regularization_gcv_ocv-evaluation-function}
\end{equation}
ただし，$\bm{x}_\lambda^{(i)}$ は $y_i$ 以外のデータから求めた解で，
$A_i$ は解からデータ $y_i$ を推定する作用素である．

ここで次の補題が成り立つ．
\begin{lemma}[文献 {\cite{Wahba1990}} の補題 4.2.1 より, leaving-out-one lemma]
    評価関数
    \begin{equation}
        \frac{1}{m} \left(\left|z - A_k \bm{x}\right|^2
        + \sum_{i \neq k} \left|y_i - A_i \bm{x}\right|^2\right)
        + \lambda R(\bm{x})
    \end{equation}
    を最小化するような $\bm{x}$ を $\bm{h}_\lambda[k,z]$ とし，
    評価関数
    \begin{equation}
        \frac{1}{m}
        \sum_{i \neq k} \left|y_i - A_i \bm{x}\right|^2
        + \lambda R(\bm{x})
    \end{equation}
    を最小化するような $\bm{x}$ を $\bm{x}_\lambda^{(k)}$ としたとき，
    $h_\lambda[k, A_k \bm{x}_\lambda^{(k)}] = \bm{x}_\lambda^{(k)}$
    が成り立つ．
\end{lemma}
\begin{proof}
    $\tilde{y}_k = A_k \bm{x}_\lambda^{(k)}$とし，
    $\bm{x} \neq \bm{x}_\lambda^{(k)}$とする．
    このとき，
    \begin{align}
         & \hphantom{=}
        \frac{1}{m} \left(\left|\tilde{y}_k - A_k \bm{x}_\lambda^{(k)}\right|^2
        + \sum_{i \neq k} \left|y_i - A_i \bm{x}_\lambda^{(k)}\right|^2\right)
        + \lambda R(\bm{x}_\lambda^{(k)})
        \notag                                                                         \\
         & = \frac{1}{m} \sum_{i \neq k} \left|y_i - A_i \bm{x}_\lambda^{(k)}\right|^2
        + \lambda R(\bm{x}_\lambda^{(k)})
        \notag                                                                         \\
         & < \frac{1}{m} \sum_{i \neq k} \left|y_i - A_i \bm{x}\right|^2
        + \lambda R(\bm{x})
        \notag                                                                         \\
         & \le \frac{1}{m} \left(\left|\tilde{y}_k - A_k \bm{x}\right|^2
        +\sum_{i \neq k} \left|y_i - A_i \bm{x}\right|^2\right)
        + \lambda R(\bm{x})
    \end{align}
    のように変形できる．
    よって，
    $h_\lambda[k, A_k \bm{x}_\lambda^{(k)}] = \bm{x}_\lambda^{(k)}$
    である．
\end{proof}

これを用い，次の量を考える．
\begin{equation}
    \tilde{p}_{kk}(\lambda)
    \equiv \frac{A_k \bm{x}_\lambda - A_k \bm{x}_\lambda^{(k)}}
    {y_k - A_k \bm{x}_\lambda^{(k)}}
\end{equation}
まず，定義から，
\begin{equation}
    y_k - A_k \bm{x}_\lambda^{(k)} =
    \frac{y_k - A_k \bm{x}_\lambda}{1 - \tilde{p}_{kk}(\lambda)}
\end{equation}
が成り立つ．
また，補題から
\begin{equation}
    \tilde{p}_{kk}(\lambda)
    = \frac{A_k h[k,y_k] - A_k h[k,\tilde{y}_k]}{y_k - \tilde{y}_k}
\end{equation}
が成り立つ．
さらに，これは 1 次近似により
\begin{equation}
    \tilde{p}_{kk}(\lambda)
    \approx \frac{\partial A_k h[k,y_k]}{\partial y_k}
    =\frac{\partial A_k \bm{x}_\lambda}{\partial y_k}
    =\frac{\partial A_k A_\lambda^\# \bm{y}}{\partial y_k}
    =p_{kk}(\lambda)
\end{equation}
と書ける．
ただし，$p_{kk}(\lambda)$ は
influence matrix $P_A(\lambda)$ の $(k,k)$ 成分である．

このことを利用して OCV の評価関数
\eqref{eq:regularization_gcv_ocv-evaluation-function}
を変形すると，
\begin{equation}
    V_0(\lambda)=
    \frac{1}{m} \sum_{i=1}^m
    \left|y_i - A_i \bm{x}_\lambda^{(i)}\right|^2
    \approx
    \frac{1}{m} \sum_{i=1}^m
    \frac{\left|y_i - A_i \bm{x}_\lambda\right|^2}
    {\left|1 - p_{kk}(\lambda)\right|^2}
\end{equation}
のように書ける．

$p_{kk}(\lambda)$
の代わりに，influence matrix の対角成分の平均値
$\tr P_A(\lambda)/n$
を用い，
$\bm{y}$
の回転に影響されない評価関数を作ると，
GCVの評価関数 \eqref{eq:regularization_gcv_gcv-evaluation-function} が得られる．

\section{Tikhonov 正則化の場合における計算方法}

Tikhonov 正則化（\ref{chap:regularization_tikhonov} 章）の場合，
残差項は式 \eqref{eq:regularization_tikhonov_residual-by-svd} で計算できる．
そこで，influence matrix の計算について考える．

\ref{chap:regularization_tikhonov} 章と同様に
行列 $A$ の特異値分解
\begin{equation}
    A = U
    \begin{pmatrix}
        \Sigma & O \\
        O      & O
    \end{pmatrix}
    V^*
\end{equation}
を行い，その場合の解（式 \eqref{eq:regularization_tikhonov_solution-by-svd-matrices}）
\begin{equation}
    x_{\lambda} = V
    \begin{pmatrix}
        (\Sigma^2 + \lambda I)^{-1} \Sigma & O \\
        O                                  & O
    \end{pmatrix}
    U^* \bm{y}
\end{equation}
を利用すると，
\begin{align}
    A A_\lambda^\# \bm{y}
     & = U
    \begin{pmatrix}
        \Sigma & O \\
        O      & O
    \end{pmatrix}
    V^* V
    \begin{pmatrix}
        (\Sigma^2 + \lambda I)^{-1} \Sigma & O \\
        O                                  & O
    \end{pmatrix}
    U^* \bm{y}
    \notag \\
     & = U
    \begin{pmatrix}
        \Sigma (\Sigma^2 + \lambda I)^{-1} \Sigma & O \\
        O                                         & O
    \end{pmatrix}
    U^* \bm{y}
\end{align}
となる．
よって，influence matrix は
\begin{align}
    P_A(\lambda)
     & = \frac{\partial A A_\lambda^\# \bm{y}}{\partial \bm{y}}
    \notag                                                      \\
     & = U
    \begin{pmatrix}
        \Sigma (\Sigma^2 + \lambda I)^{-1} \Sigma & O \\
        O                                         & O
    \end{pmatrix}
    U^*
\end{align}
であり，そのトレースは
\begin{align}
    \tr{P_A(\lambda)}
     & = \tr\left(U
    \begin{pmatrix}
            \Sigma (\Sigma^2 + \lambda I)^{-1} \Sigma & O \\
            O                                         & O
        \end{pmatrix}
    U^*\right)
    \notag                                                      \\
     & = \tr\left(
    \begin{pmatrix}
            \Sigma (\Sigma^2 + \lambda I)^{-1} \Sigma & O \\
            O                                         & O
        \end{pmatrix}
    U^* U\right)
    \notag                                                      \\
     & = \tr
    \begin{pmatrix}
        \Sigma (\Sigma^2 + \lambda I)^{-1} \Sigma & O \\
        O                                         & O
    \end{pmatrix}
    \notag                                                      \\
     & = \sum_{i = 1}^r \frac{\sigma_i^2}{\sigma_i^2 + \lambda}
\end{align}
と計算できる
\cite{Hansen1998}
\footnote{$\sigma_i^2 / (\sigma_i^2 + \lambda)$ は filter factor と呼ばれる．}．

\section{一般の場合における計算方法}

解きたい方程式や正則化項が非線形な場合，
反復的な最適化アルゴリズムで正則化した推定解を求めることになるが，
そのような場合の GCV をどのように計算すれば良いのかについて，
文献 \cite{Deshpande1991} の結果を簡単に紹介する．
なお，本節では，データ $\bm{y}$ について $\bm{y} \in \setR^m$ を前提とする．

まず，非線形な問題においてどのような問題が起きるかをまとめる．
式 \eqref{eq:regularization_gcv_gcv-evaluation-function} で示した GCV の評価関数において，
分子は推定解 $\bm{x}_\lambda$ における残差であり，計算できる場合が多い．
それに対して分母は
推定解に $A$ を作用した結果 $A A_\lambda^\#\bm{y}$ を
$\bm{y}$ について偏微分したヤコビアン $P_A(\lambda)$ を含んでおり，
非線形な問題においてはこれを解析的に計算することができない．
数値的にこれを求めることも可能だが，
その場合は全ての $i=1, 2, \ldots, m$ について
$i$ 要素目だけ1の単位行列 $\bm{e}_i$ （$i$ の要素だけ 1 の単位ベクトル）の方向へデータをずらした
$\bm{y} + \Delta y_i \bm{e}_i$ について推定解を得て
$A$ を作用させることにより，
数値微分する必要がある．
これは推定解を求めるのに時間のかかる最適化において現実的ではない．
この問題を解決するために，
文献 \cite{Deshpande1991} では次のような2段階の近似を行っている．

まず，分母を次の式で近似する．
\begin{align}
    \frac{1}{m} \tr(I - P_A(\lambda))
     & \approx \frac{\bm{w}^T (I - P_A(\lambda)) \bm{w}}{\bm{w}^T \bm{w}}
    \label{eq:regularization_gcv_influence-approximation-by-Deshpande}
\end{align}
これは文献 \cite{Girard1989} において提案された
Monte-Carlo Cross-Validation と呼ばれる手法のもので，
$\bm{w}$ は各要素を独立に平均 0，分散 1 の正規分布 $\mathcal{N}(0,1)$ から
サンプリングしたベクトルである
\footnote{この手法は線形
    （つまり $P_A(\lambda) = A A_\lambda^\#$ と行列で書ける）
    だが大規模な問題を解くために考えられたもので，
    そのような問題では，たとえ $P_A(\lambda)$ が陽的に書けても，
    トレース $\tr(I - P_A(\lambda))$ を計算するのは
    かなりコストを要するという問題があり，
    このような手法が考案された．}．
この近似式については次のような定理が証明されている．

\begin{theorem}[文献{\cite{Girard1989}} 定理 2.2]
    行列 $B \in \setR^{m \times m}$ と
    各要素が独立に正規分布 $\mathcal{N}(0,1)$ に従う
    ベクトル $\bm{w} \in \setR^m$ を用いて
    \begin{equation}
        T_B^*(\bm{w}) \equiv \frac{\bm{w}^T B \bm{w}}{\bm{w}^T \bm{w}}
    \end{equation}
    を定義する．
    このとき，$T_B^*$ の平均と分散はそれぞれ
    \begin{align}
        \mathrm{E}[T_B^*(\bm{w})]   & =\mu_1 \equiv \frac{1}{m} \tr(B) = \frac{1}{m} \sum_{i=1}^m B_{ii}
        \\
        \mathrm{Var}[T_B^*(\bm{w})] & =\frac{2}{m (m + 2)} \sum_{i=1}^m (B_{ii} - \mu_1)^2
    \end{align}
    となる．
\end{theorem}

ここで出てくる分散による誤差が気になる場合は，
複数の $\bm{w}$ で
式 \eqref{eq:regularization_gcv_influence-approximation-by-Deshpande} の近似を行った
結果の平均で $\tr(I-P_A(\lambda))/n$ の推定を行うことで
精度を上げることもできる．
しかし，文献 \cite{Girard1989} の数値実験では
$m=50$ でも真値と似たような挙動をとる推定値が得られており，
$m=500$ では真値との差をグラフから読み取るのが困難な程度に良い結果が得られている．

文献 \cite{Deshpande1991} ではさらに
\begin{align}
    P_A(\lambda) \bm{w}
     & = \frac{\partial A A_\lambda^\# \bm{y}}{\partial \bm{y}} \bm{w}
    \notag                                                                   \\
     & \approx \frac{AA_\lambda^\#(\bm{y}+h\bm{w}) - AA_\lambda^\#\bm{y}}{h}
\end{align}
と 1 次近似することにより，次の式を得ている．

\begin{align}
    \frac{1}{m} \tr(I - P_A(\lambda))
     & \approx \frac{\bm{w}^T (I - P_A(\lambda)) \bm{w}}{\bm{w}^T \bm{w}}
    \notag                                                                \\
     & = \frac{\bm{w}^T}{\bm{w}^T \bm{w}}(\bm{w} - P_A(\lambda) \bm{w})
    \notag                                                                \\
     & \approx \frac{\bm{w}^T}{\bm{w}^T \bm{w}}
    \left(\bm{w} - \frac{A A_\lambda^\#(\bm{y} + h \bm{w}) - A A_\lambda^\# \bm{y}}{h}\right)
\end{align}

数値微分のずらす幅を決める $h$ は
$A A_\lambda^\#$ の演算の精度を見ながら調整することになるが，
これにより推定解の計算を最低 2 回行えば GCV の評価関数を計算できる．
